{
  "cells": [
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "import numpy as np\nimport matplotlib.pyplot as plt \nclass RecurrentNeuralNetwork:\n    #input (word), expected output (next word), num of words (num of recurrences), array expected outputs, learning rate\n    def __init__ (self, xs, ys, rl, eo, lr):\n        #initial input (first word)\n        self.x = np.zeros(xs)\n        #input size \n        self.xs = xs\n        #expected output (next word)\n        self.y = np.zeros(ys)\n        #output size\n        self.ys = ys\n        #weight matrix for interpreting results from LSTM cell (num words x num words matrix)\n        self.w = np.random.random((ys, ys))\n        #matrix used in RMSprop\n        self.G = np.zeros_like(self.w)\n        #length of the recurrent network - number of recurrences i.e num of words\n        self.rl = rl\n        #learning rate \n        self.lr = lr\n        #array for storing inputs\n        self.ia = np.zeros((rl+1,xs))\n        #array for storing cell states\n        self.ca = np.zeros((rl+1,ys))\n        #array for storing outputs\n        self.oa = np.zeros((rl+1,ys))\n        #array for storing hidden states\n        self.ha = np.zeros((rl+1,ys))\n        #forget gate \n        self.af = np.zeros((rl+1,ys))\n        #input gate\n        self.ai = np.zeros((rl+1,ys))\n        #cell state\n        self.ac = np.zeros((rl+1,ys))\n        #output gate\n        self.ao = np.zeros((rl+1,ys))\n        #array of expected output values\n        self.eo = np.vstack((np.zeros(eo.shape[0]), eo.T))\n        #declare LSTM cell (input, output, amount of recurrence, learning rate)\n        self.LSTM = LSTM(xs, ys, rl, lr)\n    \n    #activation function. simple nonlinearity, convert nums into probabilities between 0 and 1\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n    \n    #the derivative of the sigmoid function. used to compute gradients for backpropagation\n    def dsigmoid(self, x):\n        return self.sigmoid(x) * (1 - self.sigmoid(x))    \n    \n    #lets apply a series of matrix operations to our input (curr word) to compute a predicted output (next word)\n    def forwardProp(self):\n        for i in range(1, self.rl+1):\n            self.LSTM.x = np.hstack((self.ha[i-1], self.x))\n            cs, hs, f, inp, c, o = self.LSTM.forwardProp()\n            #store computed cell state\n            self.ca[i] = cs\n            self.ha[i] = hs\n            self.af[i] = f\n            self.ai[i] = inp\n            self.ac[i] = c\n            self.ao[i] = o\n            self.oa[i] = self.sigmoid(np.dot(self.w, hs))\n            self.x = self.eo[i-1]\n        return self.oa\n   \n    \n    def backProp(self):\n        #update our weight matrices (Both in our Recurrent network, as well as the weight matrices inside LSTM cell)\n        #init an empty error value \n        totalError = 0\n        #initialize matrices for gradient updates\n        #First, these are RNN level gradients\n        #cell state\n        dfcs = np.zeros(self.ys)\n        #hidden state,\n        dfhs = np.zeros(self.ys)\n        #weight matrix\n        tu = np.zeros((self.ys,self.ys))\n        #Next, these are LSTM level gradients\n        #forget gate\n        tfu = np.zeros((self.ys, self.xs+self.ys))\n        #input gate\n        tiu = np.zeros((self.ys, self.xs+self.ys))\n        #cell unit\n        tcu = np.zeros((self.ys, self.xs+self.ys))\n        #output gate\n        tou = np.zeros((self.ys, self.xs+self.ys))\n        #loop backwards through recurrences\n        for i in range(self.rl, -1, -1):\n            #error = calculatedOutput - expectedOutput\n            error = self.oa[i] - self.eo[i]\n            #calculate update for weight matrix\n            #(error * derivative of the output) * hidden state\n            tu += np.dot(np.atleast_2d(error * self.dsigmoid(self.oa[i])), np.atleast_2d(self.ha[i]).T)\n            #Time to propagate error back to exit of LSTM cell\n            #1. error * RNN weight matrix\n            error = np.dot(error, self.w)\n            #2. set input values of LSTM cell for recurrence i (horizontal stack of arrays, hidden + input)\n            self.LSTM.x = np.hstack((self.ha[i-1], self.ia[i]))\n            #3. set cell state of LSTM cell for recurrence i (pre-updates)\n            self.LSTM.cs = self.ca[i]\n            #Finally, call the LSTM cell's backprop, retreive gradient updates\n            #gradient updates for forget, input, cell unit, and output gates + cell states & hiddens states\n            fu, iu, cu, ou, dfcs, dfhs = self.LSTM.backProp(error, self.ca[i-1], self.af[i], self.ai[i], self.ac[i], self.ao[i], dfcs, dfhs)\n            #calculate total error (not necesarry, used to measure training progress)\n            totalError += np.sum(error)\n            totalError+=(np.sum(error/self.w)/error.size)%5\n            #accumulate all gradient updates\n            #forget gate\n            tfu += fu\n            #input gate\n            tiu += iu\n            #cell state\n            tcu += cu\n            #output gate\n            tou += ou\n        #update LSTM matrices with average of accumulated gradient updates    \n        self.LSTM.update(tfu/self.rl, tiu/self.rl, tcu/self.rl, tou/self.rl) \n        #update weight matrix with average of accumulated gradient updates  \n        self.update(tu/self.rl)\n        #return total error of this iteration\n        return totalError/1000\n    \n    def update(self, u):\n        #vanilla implementation of RMSprop\n        self.G = 0.9 * self.G + 0.1 * u**2  \n        self.w -= self.lr/np.sqrt(self.G + 1e-8) * u\n        return\n    \n    #this is where we generate some sample text after having fully trained our model\n    #i.e error is below some threshold\n    def sample(self):\n         #loop through recurrences - start at 1 so the 0th entry of all arrays will be an array of 0's\n        for i in range(1, self.rl+1):\n            #set input for LSTM cell, combination of input (previous output) and previous hidden state\n            self.LSTM.x = np.hstack((self.ha[i-1], self.x))\n            #run forward prop on the LSTM cell, retrieve cell state and hidden state\n            cs, hs, f, inp, c, o = self.LSTM.forwardProp()\n            #store input as vector\n            maxI = np.argmax(self.x)\n            self.x = np.zeros_like(self.x)\n            self.x[maxI] = 1\n            self.ia[i] = self.x #Use np.argmax?\n            #store cell states\n            self.ca[i] = cs\n            #store hidden state\n            self.ha[i] = hs\n            #forget gate\n            self.af[i] = f\n            #input gate\n            self.ai[i] = inp\n            #cell state\n            self.ac[i] = c\n            #output gate\n            self.ao[i] = o\n            #calculate output by multiplying hidden state with weight matrix\n            self.oa[i] = self.sigmoid(np.dot(self.w, hs))\n            #compute new input\n            maxI = np.argmax(self.oa[i])\n            newX = np.zeros_like(self.x)\n            newX[maxI] = 1\n            self.x = newX\n        #return all outputs    \n        return self.oa",
      "execution_count": 42,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "class LSTM:\n    # LSTM cell (input, output, amount of recurrence, learning rate)\n    def __init__ (self, xs, ys, rl, lr):\n        #input is word length x word length\n        self.x = np.zeros(xs+ys)\n        #input size is word length + word length\n        self.xs = xs + ys\n        #output \n        self.y = np.zeros(ys)\n        #output size\n        self.ys = ys\n        #cell state intialized as size of prediction\n        self.cs = np.zeros(ys)\n        #how often to perform recurrence\n        self.rl = rl\n        #balance the rate of training (learning rate)\n        self.lr = lr\n        #init weight matrices for our gates\n        #forget gate\n        self.f = np.random.random((ys, xs+ys))\n        #input gate\n        self.i = np.random.random((ys, xs+ys))\n        #cell state\n        self.c = np.random.random((ys, xs+ys))\n        #output gate\n        self.o = np.random.random((ys, xs+ys))\n        #forget gate gradient\n        self.Gf = np.zeros_like(self.f)\n        #input gate gradient\n        self.Gi = np.zeros_like(self.i)\n        #cell state gradient\n        self.Gc = np.zeros_like(self.c)\n        #output gate gradient\n        self.Go = np.zeros_like(self.o)\n    \n    #activation function to activate our forward prop, just like in any type of neural network\n    def sigmoid(self, x):\n        return 1 / (1 + np.exp(-x))\n    \n    #derivative of sigmoid to help computes gradients\n    def dsigmoid(self, x):\n        return self.sigmoid(x) * (1 - self.sigmoid(x))\n    \n    #tanh! another activation function, often used in LSTM cells\n    #Having stronger gradients: since data is centered around 0, \n    #the derivatives are higher. To see this, calculate the derivative \n    #of the tanh function and notice that input values are in the range [0,1].\n    def tangent(self, x):\n        return np.tanh(x)\n    \n    #derivative for computing gradients\n    def dtangent(self, x):\n        return 1 - np.tanh(x)**2\n    \n    #lets compute a series of matrix multiplications to convert our input into our output\n    def forwardProp(self):\n        f = self.sigmoid(np.dot(self.f, self.x))\n        self.cs *= f\n        i = self.sigmoid(np.dot(self.i, self.x))\n        c = self.tangent(np.dot(self.c, self.x))\n        self.cs += i * c\n        o = self.sigmoid(np.dot(self.o, self.x))\n        self.y = o * self.tangent(self.cs)\n        return self.cs, self.y, f, i, c, o\n    \n   \n    def backProp(self, e, pcs, f, i, c, o, dfcs, dfhs):\n        #error = error + hidden state derivative. clip the value between -6 and 6.\n        e = np.clip(e + dfhs, -6, 6)\n        #multiply error by activated cell state to compute output derivative\n        do = self.tangent(self.cs) * e\n        #output update = (output deriv * activated output) * input\n        ou = np.dot(np.atleast_2d(do * self.dtangent(o)).T, np.atleast_2d(self.x))\n        #derivative of cell state = error * output * deriv of cell state + deriv cell\n        dcs = np.clip(e * o * self.dtangent(self.cs) + dfcs, -6, 6)\n        #deriv of cell = deriv cell state * input\n        dc = dcs * i\n        #cell update = deriv cell * activated cell * input\n        cu = np.dot(np.atleast_2d(dc * self.dtangent(c)).T, np.atleast_2d(self.x))\n        #deriv of input = deriv cell state * cell\n        di = dcs * c\n        #input update = (deriv input * activated input) * input\n        iu = np.dot(np.atleast_2d(di * self.dsigmoid(i)).T, np.atleast_2d(self.x))\n        #deriv forget = deriv cell state * all cell states\n        df = dcs * pcs\n        #forget update = (deriv forget * deriv forget) * input\n        fu = np.dot(np.atleast_2d(df * self.dsigmoid(f)).T, np.atleast_2d(self.x))\n        #deriv cell state = deriv cell state * forget\n        dpcs = dcs * f\n        #deriv hidden state = (deriv cell * cell) * output + deriv output * output * output deriv input * input * output + deriv forget\n        #* forget * output\n        dphs = np.dot(dc, self.c)[:self.ys] + np.dot(do, self.o)[:self.ys] + np.dot(di, self.i)[:self.ys] + np.dot(df, self.f)[:self.ys] \n        #return update gradinets for forget, input, cell, output, cell state, hidden state\n        return fu, iu, cu, ou, dpcs, dphs\n            \n    def update(self, fu, iu, cu, ou):\n        #update forget, input, cell, and output gradients\n        self.Gf = 0.9 * self.Gf + 0.1 * fu**2 \n        self.Gi = 0.9 * self.Gi + 0.1 * iu**2   \n        self.Gc = 0.9 * self.Gc + 0.1 * cu**2   \n        self.Go = 0.9 * self.Go + 0.1 * ou**2   \n        \n        #update our gates using our gradients\n        self.f -= self.lr/np.sqrt(self.Gf + 1e-8) * fu\n        self.i -= self.lr/np.sqrt(self.Gi + 1e-8) * iu\n        self.c -= self.lr/np.sqrt(self.Gc + 1e-8) * cu\n        self.o -= self.lr/np.sqrt(self.Go + 1e-8) * ou\n        return",
      "execution_count": 43,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "def LoadText():\n    #open text and return input and output data (series of words)\n    with open(\"data.txt\", \"r\") as text_file:\n        data = text_file.read()\n    text = list(data)\n    outputSize = len(text)\n    data = list(set(text))\n    uniqueWords, dataSize = len(data), len(data) \n    returnData = np.zeros((uniqueWords, dataSize))\n    for i in range(0, dataSize):\n        returnData[i][i] = 1\n    returnData = np.append(returnData, np.atleast_2d(data), axis=0)\n    output = np.zeros((uniqueWords, outputSize))\n    for i in range(0, outputSize):\n        index = np.where(np.asarray(data) == text[i])\n        output[:,i] = returnData[0:-1,index[0]].astype(float).ravel()  \n    return returnData, uniqueWords, output, outputSize, data\n\n#write the predicted output (series of words) to disk\ndef ExportText(output, data):\n    finalOutput = np.zeros_like(output)\n    prob = np.zeros_like(output[0])\n    outputText = \"\"\n    print(len(data))\n    print(output.shape[0])\n    for i in range(0, output.shape[0]):\n        v=0.0\n        for j in range(0, output.shape[1]):\n            v+=output[i][j]\n        for j in range(0, output.shape[1]):\n            if v!=0:\n                prob[j] = output[i][j] / v #np.sum(output[i])\n            else:\n                prob[j]=1/output.shape[1]\n        outputText += np.random.choice(data,p=prob)    \n    with open(\"output.txt\", \"w\") as text_file:\n        text_file.write(outputText)\n    return",
      "execution_count": 44,
      "outputs": []
    },
    {
      "metadata": {
        "scrolled": true,
        "trusted": false
      },
      "cell_type": "code",
      "source": "#Begin program    \nprint(\"Beginning\")\niterations = 100\nlearningRate = 0.001\n#load input output data (words)\nreturnData, numCategories, expectedOutput, outputSize, data = LoadText()\nprint(\"Done Reading\")\n#init our RNN using our hyperparams and dataset\nRNN = RecurrentNeuralNetwork(numCategories, numCategories, outputSize, expectedOutput, learningRate)\nt = np.zeros([iterations])\n#training time!\nfor i in range(1, iterations):\n    #compute predicted next word\n    RNN.forwardProp()\n    #update all our weights using our error\n    error = RNN.backProp()\n    #once our error/loss is small enough\n    print(\"Error on iteration \", i, \": \", error)\n    t[i-1] = error\n    if error > -100 and error < 100 or i % 100 == 0:\n        #we can finally define a seed word\n        seed = np.zeros_like(RNN.x)\n        maxI = np.argmax(np.random.random(RNN.x.shape))\n        seed[maxI] = 1\n        RNN.x = seed  \n        #and predict some new text!\n        output = RNN.sample()\n        #print(output)   \n        #write it all to disk\n        ExportText(output, data)\n        print(\"Done Writing\")\nprint(\"Complete\")\ns = np.arange(0, iterations, 1) \nplt.plot(s, t) \nplt.ylabel('Accuracy')\nplt.xlabel('No. of Iterations')\nplt.show() ",
      "execution_count": 45,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Beginning\nDone Reading\nError on iteration  1 :  495.8511574333196\nError on iteration  2 :  494.05157665763414\nError on iteration  3 :  491.95618911763603\nError on iteration  4 :  489.8877775461037\nError on iteration  5 :  487.664288373155\nError on iteration  6 :  485.9837876185557\nError on iteration  7 :  485.28627383092277\nError on iteration  8 :  484.36596687726285\nError on iteration  9 :  482.46200916254315\nError on iteration  10 :  481.1924892977981\nError on iteration  11 :  480.0665974285919\nError on iteration  12 :  479.2254948349773\nError on iteration  13 :  477.7687248814217\nError on iteration  14 :  477.2143940028341\nError on iteration  15 :  476.32199028043397\nError on iteration  16 :  474.7360259316941\nError on iteration  17 :  473.6691295019412\nError on iteration  18 :  472.38341109797415\nError on iteration  19 :  471.8433210852419\nError on iteration  20 :  470.4292287234066\nError on iteration  21 :  469.7018195719866\nError on iteration  22 :  468.2748615435241\nError on iteration  23 :  467.40414255015065\nError on iteration  24 :  466.51809670715716\nError on iteration  25 :  465.3227740989304\nError on iteration  26 :  464.0688361259322\nError on iteration  27 :  463.4715123921256\nError on iteration  28 :  462.3172496392161\nError on iteration  29 :  461.7838363443369\nError on iteration  30 :  461.18061140210233\nError on iteration  31 :  459.435982255811\nError on iteration  32 :  458.151186010926\nError on iteration  33 :  457.547804350622\nError on iteration  34 :  456.7926336410321\nError on iteration  35 :  455.47255858790453\nError on iteration  36 :  454.4328636569451\nError on iteration  37 :  453.4297493027107\nError on iteration  38 :  452.4718859013403\nError on iteration  39 :  451.85564117124176\nError on iteration  40 :  451.54416298630645\nError on iteration  41 :  449.5207685227777\nError on iteration  42 :  448.3809961631253\nError on iteration  43 :  447.9382221688618\nError on iteration  44 :  446.8887361108647\nError on iteration  45 :  445.6948622017273\nError on iteration  46 :  445.52366425367006\nError on iteration  47 :  443.8165265144191\nError on iteration  48 :  443.32599140026036\nError on iteration  49 :  441.966421193626\nError on iteration  50 :  441.06946577654475\nError on iteration  51 :  440.35021241390956\nError on iteration  52 :  439.3184544659253\nError on iteration  53 :  438.3954697578649\nError on iteration  54 :  437.251947596881\nError on iteration  55 :  436.2071961434747\nError on iteration  56 :  435.3942613472542\nError on iteration  57 :  433.9939162663734\nError on iteration  58 :  433.214725123215\nError on iteration  59 :  432.4510389482049\nError on iteration  60 :  431.57716006735865\nError on iteration  61 :  430.74809254589695\nError on iteration  62 :  429.189309024784\nError on iteration  63 :  428.126633903259\nError on iteration  64 :  427.40472667199737\nError on iteration  65 :  426.29857003576797\nError on iteration  66 :  425.6427127243425\nError on iteration  67 :  425.3754614338911\nError on iteration  68 :  423.5226176610431\nError on iteration  69 :  422.82112392380554\nError on iteration  70 :  421.7124846933578\nError on iteration  71 :  420.5878796008365\nError on iteration  72 :  420.0287818272617\nError on iteration  73 :  419.3773524042566\nError on iteration  74 :  417.81290869198875\nError on iteration  75 :  416.8174937345075\nError on iteration  76 :  416.41672842994745\nError on iteration  77 :  415.29905056680394\nError on iteration  78 :  414.5936789721702\nError on iteration  79 :  413.14678165834755\nError on iteration  80 :  412.041897906619\nError on iteration  81 :  411.63834431941143\nError on iteration  82 :  410.715564819223\nError on iteration  83 :  408.9196281696745\nError on iteration  84 :  408.5354184456014\nError on iteration  85 :  407.7415694920434\nError on iteration  86 :  406.3929038701674\nError on iteration  87 :  405.8484379743101\nError on iteration  88 :  404.2330511394089\nError on iteration  89 :  403.76537631114047\nError on iteration  90 :  402.8790830277675\nError on iteration  91 :  402.00254003218885\nError on iteration  92 :  401.02842799457204\nError on iteration  93 :  399.8460021245064\nError on iteration  94 :  399.4006657149334\nError on iteration  95 :  397.47335082409046\nError on iteration  96 :  397.16853739907873\nError on iteration  97 :  396.2543539669517\nError on iteration  98 :  395.2448673324718\nError on iteration  99 :  394.37402437536696\nComplete\n"
        },
        {
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deZCcd33n8fe3j5menvuURpoZHZZ8gWNhFCOMkzKY2mBCsCvgApYNhnhLSxUshIQQJ5Vsjk2yEJLAuki8cQIbmWUhxuFwCAG8wsQEgrGMbPmQjWRZx0gjzX3fo+/+8Tzd6tG05JGsZ3q6+/Oqmpp+fs/T3b9Hbfd3ftf3Z+6OiIgIQKzQFRARkdVDQUFERLIUFEREJEtBQUREshQUREQkK1HoCrwcLS0tvnHjxkJXQ0SkqDz++OP97t6a71xRB4WNGzeyZ8+eQldDRKSomNmRc51T95GIiGQpKIiISJaCgoiIZEUaFMzssJk9ZWZPmNmesKzJzB4yswPh78aw3MzsbjM7aGb7zOy6KOsmIiJLrURL4fXuvs3dt4fHdwG73X0rsDs8BrgF2Br+7ATuWYG6iYhIjkJ0H90K7Aof7wJuyym/zwM/AhrMrL0A9RMRKVtRBwUHvmNmj5vZzrBsjbv3AIS/28Ly9cCxnOd2h2UiIrJCol6n8Dp3P2FmbcBDZvbcea61PGVL8nqHwWUnQFdX10VV6idHh/jhwX62b2xiW2cDqWT8ol5HRKTURBoU3P1E+LvXzL4KXA+cMrN2d+8Ju4d6w8u7gc6cp3cAJ/K85r3AvQDbt2+/qM0gfvziIH/+nZ8CkIwbr1xfz41bWvi5ra28qquBZFyTskSkPFlUm+yYWTUQc/ex8PFDwB8BNwMD7v5xM7sLaHL3j5nZLwIfBN4MvAa4292vP997bN++3S92RfPw5CyPHxliz5EhHj00wBPHhjntUF0RZ8fmZm7c2sKNW1rY0laDWb5GjIhIcTKzx3Mm/ywSZUthDfDV8As1Afxfd/+WmT0G3G9mdwJHgdvD679JEBAOApPA+yKsGw3pCm6+ag03X7UGgJGpOf79hQG+f6CPHxzsZ/dzQQNmfUMVN13Ryk1XtPGzGxtpSFdEWS0RkYKKrKWwEl5OS+GlHBuc5PsH+vne873828F+JmcXANjcUs22zgY6mtK01lTQWlvJ2voq1jdU0VJToVaFiKx652spKCgsw8z8Aj85MsxPjg6x9+gw+7qH6Ruf4ex/uopEjKva69ixuYkdm5q5trOBpmq1LERkdVFQiMD8wmkGJ2bpHZvh5Mg0x4enODY4yRPHhnmye5i5heDftbm6gi1tNVzWVsOm5mo2tVTT1ZxmbX2K2sqEWhYisuIKNaZQ0hLxGG11KdrqUrxyff2ic1OzC+w9OsSzPaMc7B3np6fG+JenehianFt0XboiTldTmqvb67h6XR2XtdXQlK6gIZ2kqbqC2lRyJW9JRERBIQpVFXFu2NLCDVtaFpUPT85yqH+C40NTnByZpmdkmhf7x/nBC/18Ze/xJa9TW5lgXUMVnU1VvGJdPdd21vPKdfW01laqhSEikVBQWEEN6Qqu66rguq7GJef6x2c4MjDB8OQcQ5NzDIzP0BN2Sx3un2D3c73ZMYzqijhdzdVsbE6zta2GLWtq2dpWQ0tNJfVVSSoSWmchIhdHQWGVaKmppKWm8pznJ2bmefr4CM/2jHJkYJIjAxM8d3KMbz9zktNnDQvVVCbY3FrNVWvruKq9lvWNaVprK2mrrWRNXYp4TK0MEclPA81FbnpugUN9ExzqH2doYpbhyTkGJmY50DvG/p4xBidmF11flYxzZXstV7XXsbmlmrX1KdrDKbWttZUKGCJlQAPNJSyVjHP1umCg+mzuTt/YDCdHp+kbm+HU6EwYLEb5xpMnGJ2eX3R9ImasqUvR2VTF5tYaLmut4bLWarauqWVdfUrjGCJlQEGhhJlZdobU2dyd0al5ekan6Bme5sTIFCeGg8dHBif55309jEydmS2VmSnVVF1BY3UFrTWVbG4NpthuaKqmoTqpKbYiJUBBoUyZGfXpJPXpJFeuzd/KGJiY5YXecQ70jnOwd5zuoUmGJufYf2KUh0ens6u8M+Ixo74qSWM6SWM6WO19TUc9r+5q5Gc6GqiqUDZakdVOQUHyMrPs4PdrNjcvOe/u9I7NcKhvgmNDk4xOzYUzp4JxjcGJWfb3jPIvT58EgoARzJaqZeuaGtrqUjSlK2isTtJaU0lbbYq6KrU0RApNQUEuilkw/rCmLsVrWRo0MgYnZtl7dIgnjg3z/Mkxnj81xneeXTpjCqAyEWNTSzWXr6nl8jU1bG6toaspTVdzmjot5BNZEQoKEqmm6sXZaAHmFk4zNDnL0MQcAxMz9I0FPydHpjnUP8HjR4Z48MnFW2k0pJN0NqbpbKqiszFNR2MVHY1p1tSlaK6poDFdofUZIpeAgoKsuGQ8RlttirbaFFCb95rxmXmODExwdGCSI4OTHBuc5NjQFPt7xvh/z/Yyu3B6yXPSFXGqknFSyTittZXs2NzM67Y0s31Dk8YzRJZJ6xSk6Jw+7fSNz9A9NEnv6AwDE7MMTcwyMjXH9PwCk7MLHBucZO/RYeZPOzGDjsY0m1qq6Wyqor4qSW0qSX1Vko7GKjY0VbOuIUVCO+5JmdA6BSkpsdiZ8YzzmZiZ58eHB9l7ZIgXByZ5sX+cJ7uHGZueZ+GsQY1EzOhqTrO5pYbNrdVUVyRIJoyKeIzL19SyratB4xpSFhQUpGRVVyZ4/RVtvP6KtkXl7s7U3AJDk3McG5zk6MAkhwcmONQ3wQt94zzy074l3VNmcFlrDesagpZGXSpBV1OaK9vruHJtLW1KUiglQkFByo6Zka5IkK5IsL6hih15ptwunHbmFk4zNbvAMydG2Xt0iCe7h+kbn+XY4CTDk7OLUqFXJeO0N6RY31DFmroUrbWVtNZU0l6folMzqKSIKCiI5BGPGfFYMGh949YWbtzasuSaoYlZnj81xvMnxzg6OEnPyBTHh6Y42DtO//hMdqOljPqqJOsbqljfGMyg2tRazeaWaja2VLNWiQpllVBQELlIjdUV7NjcnLel4e4MT85xYmQqO4Pq+NAUx4eD4x/k7PsNUBGP0dFYRWdTmrV1KdbUVbKuoYrL19Zy5dpa0hX6X1VWhv5LE4mAmdEY5ol6xbr6JeczK8Jf6BvncP8kRwcnOTo4wbHBKZ7tGaU/Zw9wM+hsTLOuIRhcz6RAbw1/t9cH5amkpt3Ky6egIFIAuSvCb7hs6fm5hdP0DE/z3MlRnjs5xk9PjdE7OsPeo8OcGp1mZn7pOo3m6graG1KsrUuxtj7F1rZarumo5+r2OgUMWTYFBZFVKBmP0dUcDFD/h1esXXTO3Rmdnqd3dJpTozP0jEzRMzJNz0iwzWv30BSPHR5iZOooEIyPdDWl2dCcZkNTmnXh3hmttZW011fR1ZTWanDJUlAQKTJmQTba+qokW9fkXxHu7pwcnWZf9whPdY9wqH+cIwOT7Dk8xPjM4n00YgbrG4ONlpprKmmprqC+KklluDq8ubqC7Rsb6WhMr8TtSYEpKIiUIDOjvb6K9voqfiGnpeHuTMwuZPNNHR+e5MX+SQ73T3ByZJr9PaMMjM8u2ksjY119im1dDbTVprLbu25ormZjS5rWGq3TKBUKCiJlxMyoqUxQU5lgU0s10JT3OndnZv40M3OnOT48xWOHB/nxi4M82zPK93/az9hZrY2aygQbmtNsbKlmY3OapurK7L4aTdUVNNdU0FJTqbGNIqDcRyJywabnFjg5Ms3hgQkO909weGCSF/snODwwQffQ1JI0IhldTWmu6ajnmvX1XNZak816W12pv09XknIficgllUrGg1ZBSzVcsfjc6dPO6PQcQ+GmS4PjswxOzNI7Ns3+njH2dQ/zz/t6Fj2nIhGjtjJBdWWCNXWVbGwOXrujsYrWmkpaaitZ31Cl4LEC9C8sIpdULGY0pCtoSFewieq81wxPznJ4IJMSfZKRyTnGZ+YZn5mnZ3ia7/20j77Huxc9xww2NldzdXsdl6+pZWNLkPl2XUMVNZUJKhMxjWtcAgoKIrLiGtIVbEtXsK2z4ZzXjM/Mc3Jkir6xoJVxZGCSZ0+Msu/4MP/8VM+S6xMxoyGdZF1DFevqq9jQkua6rka2b2ikuaYyytspKQoKIrIq1VQm2NJWy5a2peem5xY4Eo5jnBqdzrYyhidnOT48zYHeMb77XC9/s3AIgI7GKhrSSaorEtSmkuFq8CBh4WWtNVy+tlYJC0ORBwUziwN7gOPu/hYz2wR8iWDaw0+AX3H3WTOrBO4DXg0MAO9w98NR109Eik8qGeeKtbVcsTb/Og2AmfkFnj4+wmOHh3j2xGg2cHQPTbL36BADE7OLrl9Xn6K1LkVDVZLGdLAG5NqOBq7pqKe+qnwCxkq0FD4M7AfqwuNPAJ9y9y+Z2f8C7gTuCX8PufsWM3tneN07VqB+IlKCKhNxXr2hiVdvyD/tdnb+NCdHglbFcyfHONg7zsDELMOTsxzsHedrT5zZJ7y2MkFdVZLaVCJcCZ7KdlOtrU+xriE4LoXEhZFOSTWzDmAX8CfArwO/BPQBa9193sxeC/yBu/+CmX07fPzvZpYATgKtfp4KakqqiERlZHKOp46PsO/4MH1jM4xOzTM6PUfv2Aw9w1P05SQtzGiurqCjKU1XU5rLWqvZ3FpDR2MVdamg26ohnaQyUfi1GoWckvpp4GOc2Z29GRh298zKl25gffh4PXAMIAwYI+H1/bkvaGY7gZ0AXV1dkVZeRMpXfTp5zr00IGhpnBqd5sTwFCdGpjgxPJ2dTbX36BDf2HdiSdCoiMfY1tnAjs1N/ExHA7WpBFUVcaorE7TXp1ZFSyOyGpjZW4Bed3/czG7KFOe51Jdx7kyB+73AvRC0FC5BVUVELlhFIkZnU5rOpvw5oabnFjg8MEHP8DRjM/OMTc9xZGCSRw8N8JmHD5JvfV9jOpndhKmzKU1nYxXrGoJ0Jesbq1ZkbCPKsPQ64K1m9mYgRTCm8GmgwcwSYWuhA8h03HUDnUB32H1UDwxGWD8RkcikknGuXFvHlWvrlpwbm57jQO8407MLTM0tMDo9x4nhoNXRPTTF86fG2P1cL7M5KdJjBl9+/2vPOUZyqUQWFNz9t4HfBghbCh9193eb2ZeBtxPMQLoD+Hr4lAfD438Pz3/3fOMJIiLFqjaV5LquxvNec/p0sBHTiZEp9h0b5g/+6Vm6h6Z49YZo61aIJOq/Bfy6mR0kGDP4bFj+WaA5LP914K4C1E1EZFWIxYy19Smu62rk9VcGizXmF6L/O3lFRjXc/XvA98LHh4Dr81wzDdy+EvURESkmiXjw9/v86aU77l1q2m5JRGSVS8aCeThzK9BSUFAQEVnlsi2FBbUURETKXiIetBTmz7FPxaWkoCAissolY8FXtbqPRETkTEtB3UciIpLIDDSr+0hERMyMRMzUUhARkUAibhpoFhGRQDIWY04tBRERgbCloNlHIiICwQI2pbkQEREgSHWhdQoiIgKELQWNKYiICARjClqnICIiQDD7SC0FEREBNPtIRERyJOIxdR+JiEggqTQXIiKSoe4jERHJSsZjzGnxmoiIAGGWVLUURESEcKBZYwoiIgKQVOpsERHJSGjxmoiIZCTiSognIiKhZEyps0VEJKR1CiIikpXU7CMREclIxDT7SEREQsEmOwoKIiJCsE6hqNNcmFnKzH5sZk+a2TNm9odh+SYze9TMDpjZP5hZRVheGR4fDM9vjKpuIiLFJhGL4Q4LEXchRdlSmAHe4O7XAtuAN5nZDuATwKfcfSswBNwZXn8nMOTuW4BPhdeJiAjB7CMg8sHmyIKCB8bDw2T448AbgAfC8l3AbeHjW8NjwvM3m5lFVT8RkWKSDINC1IPNkY4pmFnczJ4AeoGHgBeAYXefDy/pBtaHj9cDxwDC8yNAc57X3Glme8xsT19fX5TVFxFZNRKx4Os66lQXLxkUzOyDZtZ4MS/u7gvuvg3oAK4Hrsp3WeatznMu9zXvdfft7r69tbX1YqolIlJ0ktnuo8K3FNYCj5nZ/Wb2povp0nH3YeB7wA6gwcwS4akO4ET4uBvoBAjP1wODF/peIiKlKBEPWwoRz0B6yaDg7r8LbAU+C7wXOGBmf2pml53veWbWamYN4eMq4I3AfuBh4O3hZXcAXw8fPxgeE57/rrtHPylXRKQIJGLhmELELYXES18SDBqb2UngJDAPNAIPmNlD7v6xczytHdhlZnGC4HO/u3/DzJ4FvmRmfwzsJQg2hL8/b2YHCVoI77zouxIRKTHJsKUQ9eyjlwwKZvYhgr/g+4G/A37T3efMLAYcAPIGBXffB7wqT/khgvGFs8ungdsvqPYiImUisUKzj5bTUmgBftndj+QWuvtpM3tLNNUSEZFcmdlHq2GdwjfJGfA1s1ozew2Au++PqmIiInJGdp3CKph9dA8wnnM8EZaJiMgKWTWzjwDLnQXk7qdZ5gC1iIhcGsnY6lmncMjMPmRmyfDnw8ChSGslIiKLZFsKqyAovB+4AThOsMDsNcDOKCslIiKLZRPiRdx99JLdQO7ei9YMiIgUVDK2Mi2F5axTSBGktX4FkMqUu/uvRlgvERHJkV2nsAqmpH6eIP/RLwD/SpCvaCzKSomIyGLZhHirIHX2Fnf/PWDC3XcBvwhcE2mtRERkkVWTOhuYC38Pm9krCbKXboysRiIiskRihRavLWe9wb3hfgq/S5DJtAb4vUhrJSIii2QT4hVy9lGY9G7U3YeAR4DNkdZGRETyWqnU2eftPgpXL38w0hqIiMhLSqxQ6uzljCk8ZGYfNbNOM2vK/ERaKxERWWSltuNczphCZj3CB3LKHHUliYismJWafbScFc2bIq2BiIi8pJVap7CcFc3vyVfu7vdd+uqIiEg+ZkY8ZoVvKQA/m/M4BdwM/ARQUBARWUGJmBV+O053/6+5x2ZWT5D6QkREVlAyHlsVs4/ONglsvdQVERGR80vErfArms3snwhmG0EQRK4G7o+yUiIislQiFot8O87ljCn8ec7jeeCIu3dHVB8RETmHZNxWxTqFo0CPu08DmFmVmW1098OR1kxERBYJuo8KP6bwZSC3FgthmYiIrKBkLLYq9lNIuPts5iB8XBFdlUREJJ/V0lLoM7O3Zg7M7FagP7oqiYhIPolYrPCzj4D3A18ws8+Ex91A3lXOIiISnWTcCp/mwt1fAHaYWQ1g7q79mUVECiARjxW++8jM/tTMGtx93N3HzKzRzP440lqJiMgSiVj0i9eWM6Zwi7sPZw7CXdjeHF2VREQkn2Q8Fvl2nMsJCnEzq8wcmFkVUHme6zPXdZrZw2a238yeMbMPh+VNZvaQmR0IfzeG5WZmd5vZQTPbZ2bXXexNiYiUopVIc7GcoPB/gN1mdqeZ3Qk8BOxaxvPmgd9w96uAHcAHzOxq4C5gt7tvBXaHxwC3EORU2grsBO65oDsRESlxiVj0CfGWM9D8Z2a2D3gjYMC3gA3LeF4P0BM+HjOz/cB64FbgpvCyXcD3gN8Ky+9zdwd+ZGYNZtYevo6ISNlLxqNPnb3cLKknCVY1v41gP4X9F/ImZrYReBXwKLAm80Uf/m4LL1sPHMt5WndYdvZr7TSzPWa2p6+v70KqISJS1FZi9tE5WwpmdjnwTuBdwADwDwRTUl9/IW8QTmX9R+DX3H3UzM55aZ6yJSHR3e8F7gXYvn17tCFTRGQVScYKmxDvOeD7wC+5+0EAM/vIhby4mSUJAsIX3P0rYfGpTLeQmbUDvWF5N9CZ8/QO4MSFvJ+ISClLxC3y1Nnn6z56G0G30cNm9rdmdjP5/5rPy4ImwWeB/e7+lzmnHgTuCB/fAXw9p/w94SykHcCIxhNERM4Iuo8K1FJw968CXzWzauA24CPAGjO7B/iqu3/nJV77dcCvAE+Z2RNh2e8AHwfuD2cyHQVuD899k2D9w0GC3d3ed3G3JCJSmoLuo8LPPpoAvkCQ/6iJ4Ev8LuC8QcHd/41ztyxuznO9Ax94qfqIiJSrRDy2amYfAeDug+7+N+7+hqgqJCIi+a2WxWsiIrIKBJvsFD7NhYiIrAKJuOEOCxF2ISkoiIgUiWQ8+MqOcrBZQUFEpEgkYsHcnSgHmxUURESKRCJsKUSZ6kJBQUSkSCTjQUshylQXCgoiIkUiEQtbChHOQFJQEBEpEomwpRDlWgUFBRGRIlGh2UciIpKRbSlo9pGIiGTGFNRSEBGR7OwjjSmIiMiZdQqafSQiIsmY1imIiEjozIpmBQURkbKXmX0UZfpsBQURkSKRjKmlICIioTMrmtVSEBEpe9mEeFq8JiIi2YR4aimIiIgS4omISFZ2O07NPhIRkex2nGopiIhIQqmzRUQkI6nU2SIikqHZRyIikpVdp6AxBRERMTPiMVPqbBERCSRiptlHIiISSMZjxdl9ZGafM7NeM3s6p6zJzB4yswPh78aw3MzsbjM7aGb7zOy6qOolIlLMEvHi7T76e+BNZ5XdBex2963A7vAY4BZga/izE7gnwnqJiBStRKxIWwru/ggweFbxrcCu8PEu4Lac8vs88COgwczao6qbiEixSsatpKakrnH3HoDwd1tYvh44lnNdd1i2hJntNLM9Zranr68v0sqKiKw2QfdREbYULpDlKct71+5+r7tvd/ftra2tEVdLRGR1ScZiJZXm4lSmWyj83RuWdwOdOdd1ACdWuG4iIqteIl5aU1IfBO4IH98BfD2n/D3hLKQdwEimm0lERM5IxGKRzj5KRPXCZvZF4Cagxcy6gd8HPg7cb2Z3AkeB28PLvwm8GTgITALvi6peIiLFLBm3SGcfRRYU3P1d5zh1c55rHfhAVHURESkViXi0LYXVMtAsIiLLkIhF21JQUBARKSLJeKyk1imIiMjLUC7rFEREZBmKNs2FiIhceqWW5kJERF6GYPaRWgoiIgIkY1ZSaS5ERORlKLU0FyIi8jJo8ZqIiGQltXhNREQyElq8JiIiGYm4MafZRyIiAsEmO2opiIgIELQUTjucjqi1oKAgIlJEkvHga3suohlICgoiIkUkEQu2tI9qrYKCgohIEUmELQUFBRERIRkPWgrqPhIRERIxtRRERCSUyLQUIpqWqqAgIlJEMt1HUaXPVlAQESkiZ7qP1FIQESl72YFmjSmIiEi2paDZRyIiklBLQUREMpJxjSmIiEgom+ZCs49ERCST5kLrFERE5Mw6BY0piIiIZh+JiEhWWa1TMLM3mdnzZnbQzO4qdH1ERFabbOrsUm8pmFkc+CvgFuBq4F1mdnVhayUisrpkZh+VQ0vheuCgux9y91ngS8CtBa6TiMiqkiyjTXbWA8dyjrvDskXMbKeZ7TGzPX19fStWORGR1SBdGefN16ylo7EqktdPRPKqF8fylC0Jhe5+L3AvwPbt26MJlSIiq1RdKslfv/vVkb3+amopdAOdOccdwIkC1UVEpCytpqDwGLDVzDaZWQXwTuDBAtdJRKSsrJruI3efN7MPAt8G4sDn3P2ZAldLRKSsrJqgAODu3wS+Weh6iIiUq9XUfSQiIgWmoCAiIlkKCiIikqWgICIiWeZevOu/zKwPOHKRT28B+i9hdYpFOd53Od4zlOd9l+M9w4Xf9wZ3b813oqiDwsthZnvcfXuh67HSyvG+y/GeoTzvuxzvGS7tfav7SEREshQUREQkq5yDwr2FrkCBlON9l+M9Q3nedzneM1zC+y7bMQUREVmqnFsKIiJyFgUFERHJKsugYGZvMrPnzeygmd1V6PpEwcw6zexhM9tvZs+Y2YfD8iYze8jMDoS/Gwtd10vNzOJmttfMvhEebzKzR8N7/ocwNXtJMbMGM3vAzJ4LP/PXlsln/ZHwv++nzeyLZpYqtc/bzD5nZr1m9nROWd7P1gJ3h99t+8zsugt9v7ILCmYWB/4KuAW4GniXmV1d2FpFYh74DXe/CtgBfCC8z7uA3e6+FdgdHpeaDwP7c44/AXwqvOch4M6C1Cpa/xP4lrtfCVxLcP8l/Vmb2XrgQ8B2d38lQcr9d1J6n/ffA286q+xcn+0twNbwZydwz4W+WdkFBeB64KC7H3L3WeBLwK0FrtMl5+497v6T8PEYwZfEeoJ73RVetgu4rTA1jIaZdQC/CPxdeGzAG4AHwktK8Z7rgJ8HPgvg7rPuPkyJf9ahBFBlZgkgDfRQYp+3uz8CDJ5VfK7P9lbgPg/8CGgws/YLeb9yDArrgWM5x91hWckys43Aq4BHgTXu3gNB4ADaClezSHwa+BhwOjxuBobdfT48LsXPezPQB/zvsNvs78ysmhL/rN39OPDnwFGCYDACPE7pf95w7s/2ZX+/lWNQsDxlJTsv18xqgH8Efs3dRwtdnyiZ2VuAXnd/PLc4z6Wl9nkngOuAe9z9VcAEJdZVlE/Yj34rsAlYB1QTdJ+crdQ+7/N52f+9l2NQ6AY6c447gBMFqkukzCxJEBC+4O5fCYtPZZqT4e/eQtUvAq8D3mpmhwm6Bd9A0HJoCLsXoDQ/726g290fDY8fIAgSpfxZA7wReNHd+9x9DvgKcAOl/3nDuT/bl/39Vo5B4TFgazhDoYJgYOrBAtfpkgv70j8L7Hf3v8w59SBwR/j4DuDrK123qLj7b7t7h7tvJPhcv+vu7wYeBt4eXlZS9wzg7ieBY2Z2RVh0M/AsJfxZh44CO8wsHf73nrnvkv68Q+f6bB8E3hPOQtoBjGS6mZarLFc0m9mbCf6CjAOfc/c/KXCVLjkzuxH4PvAUZ/rXf4dgXOF+oIvgf6rb3f3sQayiZ2Y3AR9197eY2WaClkMTsBf4T+4+U8j6XWpmto1gcL0COAS8j+CPvpL+rM3sD4F3EMy22wv8Z4I+9JL5vM3si8BNBOmxTwG/D3yNPJ9tGBw/QzBbaRJ4n7vvuaD3K8egICIi+ZVj95GIiJyDgoKIiGQpKIiISJaCgoiIZCkoiIhIloKCFC0zczP7i5zjj5rZH0TwPp8MM3F+8qzy95rZZ8LHt13KxIpmti2cOp05fmupZvSV1UVBQYrZDPDLZtYS8fv8F+A6d//N8xw0zFcAAALPSURBVFxzG0HW3WXLWXWbzzYgGxTc/UF3//iFvL7IxVBQkGI2T7A37UfOPmFmG8xsd5hTfreZdZ3vhcIVoJ8M8/I/ZWbvCMsfJMip82imLM9zbwDeCnzSzJ4ws8vCn2+Z2eNm9n0zuzK89u/N7C/N7GHgE2Z2vZn9MExk90MzuyJcaf9HwDvC13vHWa2SvPcWvvbd4escMrO3h+XtZvZI+FpPm9nPXdS/tpSF8/2lIlIM/grYZ2Z/dlb5ZwhSCO8ys18F7ub8KZR/meCv82sJVo4+ZmaPuPtbzWzc3bed64nu/sMweHzD3R8AMLPdwPvd/YCZvQb4a4JcTACXA29094VM2mt3nzezNwJ/6u5vM7P/RrBPwAfD13vvMu+tHbgRuJIg5cEDwH8Evu3uf2LBfiLp8/w7SJlTUJCi5u6jZnYfwWYrUzmnXkvwRQ/weeDsoHG2G4EvuvsCQbKxfwV+lovIixVmpr0B+HKQdQCAypxLvhy+D0A9sMvMthJks0wu4y3Od29fc/fTwLNmtiYsewz4XJgg8Wvu/sSF3pOUD3UfSSn4NMHuWtXnueal8rnkSzl8sWIEOf235fxclXN+IufxfwceDncO+yUgdRHvl3tvuTl+DLKbtPw8cBz4vJm95yLeQ8qEgoIUvTDJ2/0s3nbxhwSZUgHeDfzbS7zMIwR9+HEzayX4Ev3xBVRjDKgN6zMKvGhmt0N2vOLaczyvnuDLGuC9+V4vjwu6NzPbQLDPxN8SZM694H17pXwoKEip+AuCsYCMDwHvM7N9wK8Q7Nucmdr5R3me/1VgH/Ak8F3gY2FK6uX6EvCb4YDxZQRf1nea2ZPAM5x7y9c/A/6Hmf2AIGtvxsPA1ZmB5rOek/fezuMm4Akz2wu8jWA/Z5G8lCVVRESy1FIQEZEsBQUREclSUBARkSwFBRERyVJQEBGRLAUFERHJUlAQEZGs/w+Jz1rroaxDKwAAAABJRU5ErkJggg==\n",
            "text/plain": "<Figure size 432x288 with 1 Axes>"
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ]
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": false
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "nbconvert_exporter": "python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.4",
      "file_extension": ".py",
      "codemirror_mode": {
        "version": 3,
        "name": "ipython"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}